## üéØ Anti-Overfitting Guide for Korean Food Classifier

### üìä Your Current Problem

```
Train Accuracy: 99.41% ‚úÖ
Val Accuracy:   77.19% ‚ùå
Gap:            22.22% ‚ö†Ô∏è TOO LARGE!
```

This is **classic overfitting** - your model is memorizing training data instead of learning generalizable features.

---

## üîß Solution: Improved Training Script

I've created `train_cnn_improved.py` with **8 anti-overfitting techniques**:

### ‚úÖ Techniques Implemented

#### 1. **Stronger Data Augmentation**
```python
# Before (weak):
- RandomRotation(10)
- ColorJitter(0.2, 0.2, 0.2)

# After (strong):
- RandomResizedCrop (scale 0.7-1.0)
- RandomRotation(15)
- RandomVerticalFlip
- ColorJitter(0.3, 0.3, 0.3, 0.1)
- RandomGrayscale
- RandomPerspective
- RandomAffine
- RandomErasing
```

#### 2. **Weight Decay (L2 Regularization)**
```python
# Before:
optimizer = optim.Adam(model.parameters(), lr=0.001)

# After:
optimizer = optim.AdamW(
    model.parameters(), 
    lr=0.0001,
    weight_decay=0.01  # L2 regularization
)
```

#### 3. **Label Smoothing**
```python
# Smooths hard labels (0 or 1) to soft labels (0.05 to 0.95)
# Prevents overconfident predictions
criterion = LabelSmoothingCrossEntropy(smoothing=0.1)
```

#### 4. **Mixup Augmentation**
```python
# Mixes two images and their labels
# Creates smoother decision boundaries
mixed_x = lam * x1 + (1 - lam) * x2
loss = lam * loss(y1) + (1 - lam) * loss(y2)
```

#### 5. **Gradient Clipping**
```python
# Prevents exploding gradients
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

#### 6. **Early Stopping**
```python
# Stops training when validation loss stops improving
early_stopping = EarlyStopping(patience=10)
```

#### 7. **Better Learning Rate Schedule**
```python
# Before: StepLR (sudden drops)
scheduler = StepLR(optimizer, step_size=7, gamma=0.1)

# After: Cosine Annealing (smooth decay)
scheduler = CosineAnnealingLR(optimizer, T_max=epochs)
```

#### 8. **Lower Initial Learning Rate**
```python
# Before: 0.001 (too high)
# After:  0.0001 (more stable)
lr = 0.0001
```

---

## üöÄ How to Use the Improved Training

### Basic Usage

```bash
python3 train_cnn_improved.py \
    --model-type resnet50 \
    --epochs 30 \
    --batch-size 32 \
    --lr 0.0001 \
    --weight-decay 0.01 \
    --patience 10
```

### Advanced Options

```bash
# For faster training (disable mixup)
python3 train_cnn_improved.py --no-mixup

# For simpler training (disable label smoothing)
python3 train_cnn_improved.py --no-label-smoothing

# Increase patience for more epochs
python3 train_cnn_improved.py --patience 15

# Different model
python3 train_cnn_improved.py --model-type efficientnet_b0
```

### Resume from Checkpoint

```bash
python3 train_cnn_improved.py \
    --resume models/cnn_trained \
    --epochs 50
```

---

## üìà Expected Results

### With Improved Training:

```
Epoch 1:  Train: 45% | Val: 42% | Gap: 3%  ‚úÖ
Epoch 5:  Train: 68% | Val: 64% | Gap: 4%  ‚úÖ
Epoch 10: Train: 82% | Val: 78% | Gap: 4%  ‚úÖ
Epoch 15: Train: 88% | Val: 84% | Gap: 4%  ‚úÖ
Epoch 20: Train: 91% | Val: 87% | Gap: 4%  ‚úÖ
```

**Target Gap: < 5%** (healthy generalization)

---

## üí° Additional Tips

### 1. **Adjust Hyperparameters Based on Gap**

| Gap | Action |
|-----|--------|
| > 20% | Increase regularization, add more augmentation |
| 10-20% | Moderate regularization needed |
| 5-10% | Good balance, minor tuning |
| < 5% | Excellent! May even reduce regularization |
| < 2% | Might be underfitting, reduce regularization |

### 2. **Hyperparameter Tuning**

```bash
# If still overfitting (gap > 10%):
python3 train_cnn_improved.py --weight-decay 0.02  # Increase

# If underfitting (val acc too low):
python3 train_cnn_improved.py --weight-decay 0.005  # Decrease

# If training is unstable:
python3 train_cnn_improved.py --lr 0.00005  # Lower LR

# If training is too slow:
python3 train_cnn_improved.py --lr 0.0002  # Higher LR
```

### 3. **Model Selection**

Different models have different tendencies:

```bash
# Most prone to overfitting (powerful):
--model-type resnet101

# Balanced:
--model-type resnet50      # Default, good choice
--model-type efficientnet_b0

# Least prone to overfitting (simpler):
--model-type mobilenet_v2
```

### 4. **Batch Size Impact**

```python
# Larger batch = more stable but may overfit faster
--batch-size 64   # Use with more regularization

# Smaller batch = noisier but better generalization
--batch-size 16   # Natural regularization effect
```

### 5. **Monitor During Training**

Watch these indicators:

```python
# Good signs:
‚úÖ Gap < 5%
‚úÖ Validation loss decreasing
‚úÖ Both accuracies increasing

# Warning signs:
‚ö†Ô∏è Gap increasing over time
‚ö†Ô∏è Validation loss increasing while train loss decreases
‚ö†Ô∏è Train acc >> val acc

# Action: Stop and add more regularization
```

---

## üéì Understanding Each Technique

### Why Each Technique Helps:

#### 1. **Data Augmentation**
- **What**: Randomly modifies training images
- **Why**: Model sees different variations, can't memorize
- **Impact**: High - most effective technique

#### 2. **Weight Decay**
- **What**: Penalizes large weights
- **Why**: Forces simpler, smoother models
- **Impact**: High - essential regularization

#### 3. **Label Smoothing**
- **What**: Softens one-hot labels (0.9 instead of 1.0)
- **Why**: Prevents overconfident predictions
- **Impact**: Medium - helps calibration

#### 4. **Mixup**
- **What**: Blends two images together
- **Why**: Creates smoother decision boundaries
- **Impact**: Medium-High - very effective

#### 5. **Gradient Clipping**
- **What**: Limits gradient magnitudes
- **Why**: Prevents unstable updates
- **Impact**: Low-Medium - stability

#### 6. **Early Stopping**
- **What**: Stops when validation stops improving
- **Why**: Prevents training too long
- **Impact**: High - prevents late overfitting

#### 7. **Learning Rate Schedule**
- **What**: Gradually reduces learning rate
- **Why**: Fine-tunes in later epochs
- **Impact**: Medium - better convergence

#### 8. **Lower Learning Rate**
- **What**: Starts with smaller steps
- **Why**: More stable, careful learning
- **Impact**: Medium - prevents overshooting

---

## üî¨ Experiment Tracking

Track your experiments:

```bash
# Experiment 1: Baseline (old script)
python3 train_cnn.py
# Result: Train 99%, Val 77%, Gap 22% ‚ùå

# Experiment 2: Improved script
python3 train_cnn_improved.py
# Expected: Train ~90%, Val ~85%, Gap ~5% ‚úÖ

# Experiment 3: Higher weight decay
python3 train_cnn_improved.py --weight-decay 0.02
# Expected: Train ~88%, Val ~86%, Gap ~2% ‚úÖ‚úÖ

# Experiment 4: Different model
python3 train_cnn_improved.py --model-type efficientnet_b0
# Expected: Similar or better results
```

---

## üìä Comparison

### Old Training vs Improved Training

| Aspect | Old | Improved | Impact |
|--------|-----|----------|--------|
| **Data Aug** | Weak | Strong | +++ |
| **Optimizer** | Adam | AdamW + WD | ++ |
| **LR** | 0.001 | 0.0001 | ++ |
| **Loss** | CrossEntropy | Label Smoothing | + |
| **Augmentation** | Basic | + Mixup | ++ |
| **Regularization** | None | Gradient Clip | + |
| **Stopping** | Fixed epochs | Early Stopping | ++ |
| **Scheduler** | StepLR | CosineAnnealing | + |
| **Expected Gap** | 20%+ | <5% | ‚úÖ |

---

## üéØ Quick Start

1. **Use the improved script:**
```bash
python3 train_cnn_improved.py
```

2. **Monitor the gap:**
```python
If gap > 10%: Add more regularization
If gap < 5%:  Good! Continue training
If gap < 2%:  May reduce regularization slightly
```

3. **Stop early if needed:**
- Script will auto-stop if validation stops improving
- Default patience: 10 epochs

4. **Compare results:**
```bash
# Old: Gap ~22%
# New: Gap ~5% (goal)
```

---

## ‚úÖ Expected Outcome

After using the improved training:

```
Before:
‚úó Train: 99.41%, Val: 77.19%, Gap: 22.22%

After:
‚úì Train: 90.5%, Val: 86.2%, Gap: 4.3%  ‚Üê MUCH BETTER!
```

The gap should be **< 5%** for good generalization.

---

## üîç Debugging Guide

### If gap is still > 10%:

1. **Add more dropout** (modify CNN classifier):
```python
# In src/cnn_classifier.py, add dropout layers
model.classifier = nn.Sequential(
    nn.Dropout(0.3),  # Add this
    nn.Linear(features, num_classes)
)
```

2. **Increase weight decay**:
```bash
--weight-decay 0.02  # or 0.03
```

3. **Use smaller model**:
```bash
--model-type mobilenet_v2
```

4. **Reduce batch size**:
```bash
--batch-size 16
```

### If validation accuracy is too low (< 80%):

1. **Train longer** (if not overfitting):
```bash
--epochs 40 --patience 15
```

2. **Reduce regularization**:
```bash
--weight-decay 0.005
```

3. **Use larger model**:
```bash
--model-type resnet101
```

4. **Check data quality**:
- Are images clear?
- Are labels correct?
- Is dataset balanced?

---

## üéâ Summary

**Use `train_cnn_improved.py` for better results!**

It includes:
- ‚úÖ 8 anti-overfitting techniques
- ‚úÖ Automatic early stopping
- ‚úÖ Better hyperparameters
- ‚úÖ Gap monitoring
- ‚úÖ Best practices

**Expected improvement: 77% ‚Üí 85%+ validation accuracy**

Good luck with training! üöÄ


